{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cdef481",
   "metadata": {},
   "source": [
    "# Positive and Unlabeled Learning\n",
    "### simulating unlabeled data in the breast cancer data set, then building a classifer using propensity\n",
    "\n",
    "Uses Selected at Random Assumption and propensity. Adapted from \"Machine Learning from Weak Supervision\"\n",
    "https://mitpress.mit.edu/9780262047074/machine-learning-from-weak-supervision/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d74a16",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04ecfa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report,f1_score,accuracy_score,roc_auc_score\n",
    "\n",
    "\n",
    "# Propensity Ops functions\n",
    "from propensity_ops import estimate_propensity_scores, compute_class_weights, train_pu_model, calculate_optimal_threshold\n",
    "\n",
    "# Set the random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06153a80",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1575ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def determine_column_type(df):\n",
    "    categorical_columns = []\n",
    "    continuous_columns = []\n",
    "\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            categorical_columns.append(column)\n",
    "        elif pd.api.types.is_numeric_dtype(df[column]):\n",
    "            unique_values = df[column].nunique()\n",
    "            if unique_values < 10:  # Adjust this threshold as needed\n",
    "                categorical_columns.append(column)\n",
    "            else:\n",
    "                continuous_columns.append(column)\n",
    "        else:\n",
    "            # Handle other data types if necessary\n",
    "            pass\n",
    "\n",
    "    return categorical_columns, continuous_columns\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b5838e",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4310ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('malware.csv', sep =\"|\")\n",
    "categorical_columns, continuous_columns = determine_column_type(data)\n",
    "\n",
    "df_categorical = data[categorical_columns]\n",
    "df_continuous = data[continuous_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6839b4e6",
   "metadata": {},
   "source": [
    "# scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "baf6fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the continuous DataFrame\n",
    "df_continuous_scaled = pd.DataFrame(scaler.fit_transform(df_continuous), columns=df_continuous.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "730b93c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= data['legitimate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c2e19a",
   "metadata": {},
   "source": [
    "# split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719675b",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y =df_continuous_scaled ,data['legitimate']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e71582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (110437, 49)\n",
      "Validation set size: (13805, 49)\n",
      "Testing set size: (13805, 49)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and temporary sets (80% training, 20% temporary)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(df_continuous_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the temporary set into validation and testing sets (50% validation, 50% testing)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Validation set size:\", X_val.shape)\n",
    "print(\"Testing set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53526674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    77474\n",
       "1    32963\n",
       "Name: legitimate, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268d32c",
   "metadata": {},
   "source": [
    "# hide 80 % of the labels so unlabeled and negative labels are grouped together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77ea64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.rand(len(X_train)) < 0.75\n",
    "y_train = y_train.values\n",
    "y_train[mask]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52044482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "maskv = np.random.rand(len(X_val)) < 0.75\n",
    "y_val = y_val\n",
    "y_val[maskv]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ecd79f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    102371\n",
       "1      8066\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3586ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a99669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04024aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c575786b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "### subgroup will be defined as (df['mean radius'] > df['mean radius'].median()) & (df['mean texture'] > df['mean texture'].median())\n",
    "### we will be using mean radius and mean texture as the features to define the subgroup specifically where the mean exceeds the median\n",
    "\n",
    "\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "# Create a boolean mask for the subgroup\n",
    "# Adjust the conditions as needed to define your subgroup\n",
    "subgroup_mask = (df['mean radius'] > df['mean radius'].median()) & (df['mean texture'] > df['mean texture'].median())\n",
    "\n",
    "# Create a new column 'subgroup' to track the subgroup\n",
    "df['subgroup'] = subgroup_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2b8a57",
   "metadata": {},
   "source": [
    "# Mask some of the labels from the subgroup\n",
    "# Here we randomly mask 50% of the subgroup labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dcb656",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "\n",
    "mask = np.random.rand(len(df)) < 0.5\n",
    "df.loc[subgroup_mask, 'target'] = df.loc[subgroup_mask, 'target'].mask(mask[subgroup_mask])\n",
    "\n",
    "# Set all other labels outside the subgroup to 0\n",
    "df.loc[~subgroup_mask, 'target'] = 0\n",
    "\n",
    "# Hold back the true labels for all instances\n",
    "df['true_labels_all'] = data.target\n",
    "\n",
    "# Hold back the true labels for just the subgroup\n",
    "df['true_labels_subgroup'] = df['target'].where(subgroup_mask, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e3f7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T19:47:02.701115Z",
     "start_time": "2024-03-19T19:47:02.471323Z"
    }
   },
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbb5de3",
   "metadata": {},
   "source": [
    "# Setup X, Y training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2d38f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:39:25.719611Z",
     "start_time": "2024-03-19T14:39:25.676261Z"
    }
   },
   "source": [
    "# make a copy of the original DF\n",
    "\n",
    "df_bak = df.copy()\n",
    "\n",
    "\n",
    "# Define your features and target\n",
    "X = df.drop(['target', 'subgroup', 'true_labels_all', 'true_labels_subgroup'], axis=1)\n",
    "y = df['target'].fillna(0)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data and store the indices\n",
    "indices = np.arange(len(X))\n",
    "train_indices, test_indices, X_train, X_test, y_train, y_test = train_test_split(indices, X_scaled, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09220a28",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df2f4d57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:39:25.719611Z",
     "start_time": "2024-03-19T14:39:25.676261Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drdre\\.conda\\envs\\tensorflowcpu_20220413\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\drdre\\.conda\\envs\\tensorflowcpu_20220413\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Estimate propensity scores\n",
    "propensity_scores = estimate_propensity_scores(X_train, y_train)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weights(y_train, propensity_scores)\n",
    "\n",
    "# Train the PU model\n",
    "pu_model = train_pu_model(X_train, y_train, class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b09600e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=pu_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f5d9fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06643274853801169, 0.7109018471568272)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred),accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44e82260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9672,    0],\n",
       "       [3991,  142]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "93604ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probsv=pu_model.predict_proba(X_val)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a8640fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs=pu_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd9d185",
   "metadata": {},
   "source": [
    "\n",
    "# Function to find the optimal threshold\n",
    "def find_optimal_threshold(y_true, y_probs):\n",
    "    thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "    f1_scores = [f1_score(y_true, y_probs >= t) for t in thresholds]\n",
    "    optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    return optimal_threshold, max(f1_scores)\n",
    "\n",
    "# Find the optimal threshold\n",
    "optimal_threshold, best_f1_score = find_optimal_threshold(y_test, y_probs)\n",
    "print(\"Optimal threshold:\", optimal_threshold)\n",
    "print(\"Best F1 score:\", best_f1_score)\n",
    "\n",
    "# Apply the optimal threshold to make final predictions\n",
    "y_preds = (y_probs >= optimal_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57745521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold: 0.08\n",
      "Best F1 score: 0.36597049243150026\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to find the optimal threshold\n",
    "def find_optimal_threshold(y_true, y_probs):\n",
    "    thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "    f1_scores = [f1_score(y_true, y_probs >= t) for t in thresholds]\n",
    "    optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    return optimal_threshold, max(f1_scores)\n",
    "\n",
    "# Find the optimal threshold\n",
    "optimal_threshold, best_f1_score = find_optimal_threshold(y_val, y_probsv)\n",
    "print(\"Optimal threshold:\", optimal_threshold)\n",
    "print(\"Best F1 score:\", best_f1_score)\n",
    "\n",
    "# Apply the optimal threshold to make final predictions\n",
    "y_preds = (y_probs >= optimal_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c00a879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9283451133317085"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "579397da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9574067366896052"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce34cd",
   "metadata": {},
   "source": [
    "# AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dfe05319",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs=pu_model.predict_proba(X_test)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "176e8bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9853668510047535"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test,y_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9481a74",
   "metadata": {},
   "source": [
    "# make proba preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2884e104",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:39:25.719611Z",
     "start_time": "2024-03-19T14:39:25.676261Z"
    }
   },
   "outputs": [],
   "source": [
    "y_probs=pu_model.predict_proba(X_test)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be753feed991bc5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:39:46.138443Z",
     "start_time": "2024-03-19T14:39:46.129464Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_bak' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, classification_report\n\u001b[1;32m----> 2\u001b[0m df_test\u001b[38;5;241m=\u001b[39m\u001b[43mdf_bak\u001b[49m\u001b[38;5;241m.\u001b[39mloc[test_indices]\u001b[38;5;66;03m#['subgroup']\u001b[39;00m\n\u001b[0;32m      3\u001b[0m df_sub_test\u001b[38;5;241m=\u001b[39mdf_test[df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubgroup\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_bak' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "df_test=df_bak.loc[test_indices]#['subgroup']\n",
    "df_sub_test=df_test[df_test['subgroup']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f19740da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf_test\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e4f12a",
   "metadata": {},
   "source": [
    "## Find Optimal Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88fd1da475294161",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:39:25.719611Z",
     "start_time": "2024-03-19T14:39:25.676261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold: 0.050754192309385875\n"
     ]
    }
   ],
   "source": [
    "optimal_threshold = calculate_optimal_threshold(y_test, y_probs)\n",
    "print(\"Optimal threshold:\", optimal_threshold)\n",
    "y_preds=(y_probs>=optimal_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "785ca65720323714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:48:35.190634Z",
     "start_time": "2024-03-19T14:48:35.181724Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_sub_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_true_sub\u001b[38;5;241m=\u001b[39m\u001b[43mdf_sub_test\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_labels_all\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_sub_test' is not defined"
     ]
    }
   ],
   "source": [
    "y_true_sub=df_sub_test['true_labels_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "599757a55b4336dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:50:47.269533Z",
     "start_time": "2024-03-19T14:50:47.262234Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_preds_sub0\u001b[38;5;241m=\u001b[39my_preds[\u001b[43mdf_test\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubgroup\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "y_preds_sub0=y_preds[df_test['subgroup']].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0165378a",
   "metadata": {},
   "source": [
    "# Confusion Matrix using Optimal Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ffe63f4a161ce5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:51:06.936496Z",
     "start_time": "2024-03-19T14:51:06.931966Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true_sub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m confusion_matrix(\u001b[43my_true_sub\u001b[49m, y_preds_sub0)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_true_sub' is not defined"
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_true_sub, y_preds_sub0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c000d42a7cecff67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:55:15.197206Z",
     "start_time": "2024-03-19T14:55:15.190001Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true_sub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(\u001b[43my_true_sub\u001b[49m, y_preds_sub0))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_true_sub' is not defined"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_sub, y_preds_sub0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f86fdc",
   "metadata": {},
   "source": [
    "# train out of box LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b48daf42",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m----> 3\u001b[0m clf \u001b[38;5;241m=\u001b[39m LogisticRegression(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      5\u001b[0m y_lr_proba\u001b[38;5;241m=\u001b[39mclf\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:,\u001b[38;5;241m1\u001b[39m:][df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubgroup\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train.values.reshape(-1, 1))\n",
    "\n",
    "y_lr_proba=clf.predict_proba(X_test)[:,1:][df_test['subgroup']]\n",
    "#clf.score(X_test, y_test.values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf1541",
   "metadata": {},
   "source": [
    "# AUC for PU learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "359d01a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111111111111111"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_true_sub,y_probs[df_test['subgroup']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e14182",
   "metadata": {},
   "source": [
    "# AUC for Standard LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed6b9824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9055555555555556"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_true_sub,y_lr_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3962c26c",
   "metadata": {},
   "source": [
    "# get Optimal Threshold for LR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a92b6fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:54:04.375854Z",
     "start_time": "2024-03-19T14:54:04.369491Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get the test instances that belong to the subgroup\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_test_subgroup \u001b[38;5;241m=\u001b[39m X_test[\u001b[43mdf_test\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubgroup\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      3\u001b[0m y_test_subgroup \u001b[38;5;241m=\u001b[39m y_test[df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubgroup\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Get the predicted probabilities for the subgroup\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the test instances that belong to the subgroup\n",
    "X_test_subgroup = X_test[df_test['subgroup']]\n",
    "y_test_subgroup = y_test[df_test['subgroup']]\n",
    "\n",
    "# Get the predicted probabilities for the subgroup\n",
    "y_probs_subgroup = clf.predict_proba(X_test_subgroup)[:, 1:]\n",
    "\n",
    "# Calculate the optimal threshold for the subgroup\n",
    "optimal_threshold_subgroup = calculate_optimal_threshold(y_test_subgroup, y_probs_subgroup)\n",
    "print(\"Optimal threshold for subgroup:\", optimal_threshold_subgroup)\n",
    "\n",
    "# Get the predictions for the subgroup using the new threshold\n",
    "y_preds_subgroup = (y_probs_subgroup >= optimal_threshold_subgroup).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa474415",
   "metadata": {},
   "source": [
    "# confusion Matrix for LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "67df56b5b6aceb7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:54:40.421049Z",
     "start_time": "2024-03-19T14:54:40.415369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39, 19],\n",
       "       [ 0,  6]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test_subgroup, y_preds_subgroup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8273af",
   "metadata": {},
   "source": [
    "# confusion Matrix for PU Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c072342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:51:06.936496Z",
     "start_time": "2024-03-19T14:51:06.931966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39, 15],\n",
       "       [ 0, 10]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true_sub, y_preds_sub0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4337f",
   "metadata": {},
   "source": [
    "#  Brief Discussion\n",
    "\n",
    "The SAR propensity model modestly outperforms standard logistic regression, based on AUC, which translates naturally to Confusion Matrix when using optimal thresholds. However, it outperforms in an interesting and important way. The detection of positives is superior by a significant margin (~40%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3fcbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
